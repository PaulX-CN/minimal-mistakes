---
layout: single
title: Which MTA station should you assign people to?
modified:
categories: blog/
excerpt:
tags: [classfication], [NLP]
image:
    feature:
header:
    overlay_image: 
date: 2017-02-19T18:35:45-00:00
--- 

The complete project code can be found at [Github](https://github.com/PaulX-CN/StackOverflowPython)

### Reading the Data and Cleaning

Recently StackoverFlow team on Kaggle posted a dataset of all questions tagged with Python and all responding answers [Here is the link](https://www.kaggle.com/stackoverflow/pythonquestions).  There is also the other version of all R questions and surely the entire dataset of all questions posted on StackoverFlow.

The dataset, including over 600k rows, comprised of three parts, questions, answers, and tags that were attached to each question. Unlike other natural language corpus, both the question and answer body included **large portion of codes**, which produced a lot of difficulties for our parsing and understanding task.

The way I deal with it is to extract the code part (they were all surrounded by <\code> tags) and break down the body into two parts: question body and code body. I built my model based on the main question body but I used codes for one of my info resource.

```python
Id	OwnerUserId	CreationDate	Score	Title	QuestionBody	CodeBody
0	469	147	2008-08-02T15:11:16Z	21	How can I find the full path to a font from it...	I am using the Photoshop's javascript API to ...	NaN
1	502	147	2008-08-02T17:01:58Z	27	Get a preview JPEG of a PDF on Windows?	I have a cross-platform (Python) application ...	NaN
2	535	154	2008-08-02T18:43:54Z	40	Continuous Integration System for a Python Cod...	I'm starting work on a hobby project with a p...	NaN
```

The ultimate goal of this project is to classify whether a question is good or bad, in other words, whether it is going to have a positive vote or a negative vote.  

Other than the TF-IDF, I generated some other features from the question body. One of them is the sum of TF of title verbs and nouns. The ideology behind is a good title should be a good summarization of your question and same for the tags. The other features included, whether you posted a link, whether you asked a question, whether you said thanks at the end, etc.

### Tools I used  

- PostgreSQL on AWS

> In this Kaggle project, I imported all the data into postgreSQL on my AWS server to speed up my data exploration. It was much easier to do table join than in python. Other than that, it does not need to use any of my local memory, otherwise, large load of computation would easily kill my kernel.  

- Pandas (why would I even list this?)
- Sklearn
  - Pipeline
  - FeatureUnions
  - CountVectorizer + TfidfVectorizer
  - SGD classifier
  - GridSearchCV + RandomSearchCV

For this project, I used the traditional method of Bag of words to train the classifier. However, it is always difficult to combine the TF-IDF info with other features of the dataset. I was inspired by [This Post](http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html) and learnt to use to FeatureUnions to deal with it.

### Working Pipeline

Using a Pipeline simplifies this process of gathering different features from the dataset. It clearly describes the working flow of your data (perhaps you want to lay it out nicely). For example, below is the pipeline I used:  

```python  
pipeline = Pipeline([
    ('features', FeatureUnion(
        transformer_list = [
        ('stats', Pipeline([
                ('extract', GetItemTransformer(columns)),
                ('substractK', SelectKBest(k=5))])),
        ('title',Pipeline([
            ('extract', GetItemTransformer(['Title'])),
            ('count', CountVectorizer())])),
        ('question', Pipeline([
            ('extract', GetItemTransformer(['QuestionBody'])),
            ('tfidf', CountVectorizer())])),],
    # the weight was trained separately,
    # I also controlled the weight to be fairly equal assigned.
    transformer_weights={
            'stats': 0.4,
            'title':0.2,
            'Question': 0.4
        })),
    ('scaler',Normalizer()),    
    ('estimators', SGDClassifier(alpha=0.001,loss='modified_huber',penalty='l2')),
])
```

This pipeline consists of three parts:

- Gathering basic stats/features of the question body and select 5 top features
- Convert Question title and body to CountVector
- Join all three parts and normalize
- fit into classifier

A trick of using pipeline is that, you can easily embed a sub-pipeline into the main work flow. In the example above, The first step of gathering features have three parts and each part consist of a sub-pipeline. It does worth to point out that essentially everything is the pipeline follows a 'fit-transform' model. What it means is that you can design any transformer to add to this pipeline. However, they must be built based on two base class BaseEstimator,TransformerMixin, otherwise they will raise selector Error.

> - One trick is that, how the pipeline work beneath is that it calls Fit_transform on your train data (if you are using tf-idf, this is where you learn your idf) and then only applies transform on the test data. This is extremely handy because **test data should only use the idf from your train dataset**.
> - Another trick is that the feature Union function accept parameter of  **transformer_weights** , which allows you to assign weight to each class.   

### Model Fitting

One of the benefit using pipeline is now you have tons of parameters to tune! (Isn't that good?) What makes it even better is you can just hand the pipeline to GridSearch and let it run whole night before you go to sleep!  

```python  
param_grid = dict(features__title__count__min_df=np.linspace(0.03,0.07,num=5),
                  features__title__count__max_df=[0.6,0.7,0.8],
                features__question__tfidf__min_df=np.linspace(0.01,0.05,num=5),
                  features__question__tfidf__max_df=[0.4,0.5,0.6,0.7,0.8],
                  estimators__alpha= [1e-3,1e-2,1e-1],
                  estimators__penalty= ['l1','l2'],
                estimators__loss=['log','modified_huber'],
                 )
```

There are few challenges in this project. One of them is the imbalance class. If I assign questions score above 0 to be good and below 0 to be bad, that gives us about 20:1 ratio. So I started with downsampling, by filtering out the questions scored between 0 and 5, bring the ratio down to 1.5:1.

The goal is to maximize the recall on 'Bad questions' so I assigned positive label to Bad and passed a recall scorer to grid search.  

```python  
recall_scorer = make_scorer(recall_score, pos_label="Bad")

grid_search = RandomizedSearchCV(pipeline, param_grid, verbose=5, scoring=recall_scorer,n_jobs=5,n_iter=50)
```   

By the time I apply the model back to the original dataset, I had to add a class weight to the model. The class weight is essentially a weighted penalty to two different classes.  I assigned 15:1 penalty weight since the original class ratio is about 1:16 but it was actually tested with grid search also.  

```python  
SGDClassifier(alpha=0.001,loss='modified_huber',penalty='l2',class_weight={'Bad':15, 'Good':1}
```  

## Final Thoughts

The final model reached 80% recall on 'Bad' class but the precision was low at about 23%. After looking into the test dataset, I realize that my model still makes a lot of sense. For example, below is a [False Positive Sample](http://stackoverflow.com/questions/5306756/how-to-show-percentage-in-python). This is very old question from 2011. I am pretty sure that if this question is asked today, it will have a very bad score. So one thing my project did not take into consideration is the time series issue.    

On contrary, this is a [False negative example](http://stackoverflow.com/questions/23588043/find-an-email-address-on-a-web-page-using-a-regular-expression). The author gave his code and states the problem clearly yet he had a -1 score. The major problem here is the ambiguity of words. One way to deal with it is to use Word2Vec to train and deambiguition.  

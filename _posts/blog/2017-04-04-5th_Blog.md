---
layout: single
title: Automation of Understanding Daily News
modified:
categories: blog
tags: [Word2Vec, TopicModeling, NER]
date: 2017-04-3T17:35:45-00:00
--- 

For my fourth project at Metis, I decided to dive deeper into the NLP, trying to cover most common NLP techiniques. In this project, I used Topic Modeling such as NMF and LDA, Word2Vec, a deep learning tool for word embedding and NER, a tool to understand the facts.

Other than exploring the techiniques, I aimed to build a pipeline that will automate the whole process. You will see working examples below.

The final Code find be found [here]().

The project has four parts:
- Automated web crawler that download and store daily news
- Train Word2Vec
- Topic Modeling using NMF and LDA
- Visualization the results using D3.JS
- Wrap up all the functions to build a automated pipeline 

## Webcrawler

I used **[NewsPaper](http://newspaper.readthedocs.io/en/latest/)** package to build a automated Web Crawler, which will download news articles from all major news websites every 4 hours.

I used **PostgreSQL** to store all the news since I am certain on what fields I will get from crawler.

## Word2Vec training

The pipeline was set up to pre-train word2vec model on daily news once the downloading is over. 

>To train Word2Vec, I added Bigrams to the vocabulary because I want to catch the most frequent human names or organizations. In the meanwhile, I need to raise the minimum apperance limit to filter out most confusing bigrams. 

I found that with a day volume of news, word2vec model can achieve pretty good results on single word query and words postive combination. However, negative combination does not show any good results probably due to the limited size of daily news.

I have tried two visulization methods of TSNE and PCA, both of which were implemented in the final module.

Finally, I built an interactive scatter plot website using d3js for users to play around with queries, which will return the 10 most relevent words. Below is a graph of the final outcome:

![final]()

## Topic Modeling in NMF and LDA

I tried few variation of topic Modeling, using TF-IDF or CountVector and NMF or LDA.  

Comparing with CountVector, TF-IDF removes the domain-frequent words. This is helpful if we are only looking at certain area. However, since news consist of a lot of different topics and angles, it is not very necessary as long as we removed the stopword. 
  
I have not observed much difference between NMF and LDA since everyday I am re-running the process. However, theoratically there is slightly difference between two techniques, I quote from one of the answers from Quora below:  

>The only difference is that LDA  adds a Dirichlet prior on top of the data generating process, meaning NMF qualitatively leads to worse mixtures. It fixes values for the probability vectors of the multinomials, whereas LDA allows the topics and words themselves to vary.
Thus, in cases where we believe that the topic probabilities should remain fixed per document (often times unlikely)—or in small data settings in which the additional variability coming from the hyperpriors is too much—NMF performs better.


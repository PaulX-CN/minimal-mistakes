---
layout: single
title: Automation of Understanding Daily News
modified:
categories: blog
tags: [Word2Vec, TopicModeling, NER]
date: 2017-04-3T17:35:45-00:00
--- 

For my fourth project at Metis, I decided to dive deeper into the NLP, trying to cover most common NLP techiniques. Other than exploring the techiniques, I aimed to build a pipeline that will automate the whole process, practicing my python coding skills. You will see working examples below.

The final Code find be found [here](https://github.com/PaulX-CN/project_folder/tree/master/DailyNews).

The project has five parts:
- Automated web crawler that download and store daily news
- Train Word2Vec
- Topic Modeling using NMF and LDA
- Visualization the results using D3.JS
- Wrap up all the functions to build a automated pipeline 

I have also tried to use clustering(KMeans and DBScan) on W2V results but they did not give very good results.

## Webcrawler

I used **[NewsPaper](http://newspaper.readthedocs.io/en/latest/)** package to build a automated Web Crawler, which will download news articles from all major news websites every 4 hours.

I used **PostgreSQL** to store all the news since I am certain on what fields I will get from crawler.

## Word2Vec training

The pipeline was set up to pre-train word2vec model on daily news once the downloading is over. 

>To train Word2Vec, I added Bigrams to the vocabulary because I want to catch the most frequent human names or organizations. In the meanwhile, I need to raise the minimum apperance limit to filter out most confusing bigrams. 

I found that with a day volume of news, word2vec model can achieve pretty good results on single word query and words postive combination. However, negative combination does not show any good results probably due to the limited size of daily news.

I have tried two visulization methods of TSNE and PCA, both of which were implemented in the final module.

Finally, I built an interactive scatter plot website using d3js for users to play around with queries, which will return the 10 most relevent words. 

## Topic Modeling in NMF and LDA

I tried few variation of topic Modeling, using TF-IDF or CountVector and NMF or LDA.  

Comparing with CountVector, TF-IDF removes the domain-frequent words. This is helpful if we are only looking at certain area. However, since news consist of a lot of different topics and angles, it does not make a big difference as long as we removed the most common stopword. 
  
I have not observed much difference between NMF and LDA since everyday I am re-running the process. However, theoratically there is slightly difference between two techniques, I quote from one of the answers from Quora below:  

>The only difference is that LDA  adds a Dirichlet prior on top of the data generating process, meaning NMF qualitatively leads to worse mixtures. It fixes values for the probability vectors of the multinomials, whereas LDA allows the topics and words themselves to vary.
Thus, in cases where we believe that the topic probabilities should remain fixed per document (often times unlikely)—or in small data settings in which the additional variability coming from the hyperpriors is too much—NMF performs better.

## Visualization

The website was built using D3, below is the main page design.

![main page](/assets/images/OpeningSearch.png)

The 1st image users will see is a Topic Modeling based on last 24 hours news with default 10 topics. Users are then able to search for any words they are interested in. For any keyword, user can tune the topic numbers to better their understanding of different angles of the news surrounding that keyword.

For example, this is a query on 'Trump'. 

![TrumpSearch](/assets/images/TrumpSearch.png)

After understanding certain topics, user can then navigate to the word understanding page. For example, if a user sees the topic 'mexico' appears often with Trump recently, he can then query 'Trump + Mexcio' and have a better understanding. Below is the example:

![TrumpMexico](/assets/images/Trump&Mexico.png)

To find out the relationship among few bunch of words, the user can query like 'Trump, Mexico, Women', using comma to seperate the word:

![MulWords](/assets/images/multiquery.png)

## NER

I tried two common NER tools,  Stanford Named Entity Recognizer (NER) toolkit, polyglot. 

My original intention was to see if I am able to extract facts from all these news so users are able to find out who are involved in their interesting topics. This could be done right after pre-training the w2v model. However, it needs a lot of tuning when I actually try to implement in production code. 

Generally, Stanford NER has higher accuracy and polyglot has higher recall.

## Clustering on Word2Vec results

When I was researching for ideas for my project, I read an interesting paper of building the domain-specific sentiment dictionary. For example, sentiment word like Angry may be close to Mexico Wall, etc. However, probably due to the size of training data, I did not see any notable results.

## Summary

The project is more like a learning process of how to actually run Topic Modeling and train a Word2Vec model. By the time I decided to take on this project, I had some understanding of both python coding and general NLP theories and then decided to make a real business-like project.

Hope you enjoyed this project and find it useful!

---
layout: single
title: Which MTA station should you assign people to?
modified:
categories: blog/
excerpt:
tags: [pandas]
image:
    feature:
header:
    overlay_image: /assets/images/500.jpg
date: 2017-01-12T10:35:45-00:00
--- 

Recently Metis received a new project from `WomenTechWomenYes (WTWY)` with blow request:

>WomenTechWomenYes (WTWY) has an annual gala at the beginning of the summer each year. As we are new and inclusive organization, we try to do double duty with the gala both to fill our event space with individuals passionate about increasing the participation of women in technology, and to concurrently build awareness and reach.  
>To this end we place street teams at entrances to subway stations. The street teams collect email addresses and those who sign up are sent free tickets to our gala.  
>Where we’d like to solicit your engagement is to use MTA subway data, which as I’m sure you know is available freely from the city, to help us optimize the placement of our street teams, such that we can gather the most signatures, ideally from those who will attend the gala and contribute to our cause.

The request specifically mentioned about dataset from NYC MTA subway data since the major task here is to find out the best location for street team placement, possibly with the busiest traffic. Therefore, even though there are many other available data sources in the scenario, due to time constrain, our first project was mostly based upon of the MTA data.   

### Data Exploration  

Cleaning the MTA data is much harder than expected and most of time we found new issues with the dataset when we started data visualizations.   

The few things we noticed in the first places included:

- The entry/exit count is an **accumulative number** which needs to be normalize by each count.  

- Each station has at least one but most stations have few turnstiles, each turnstile has its Control Area code and SCP ID.  

- Some counts were **reset to be 0** when previous count is above 1 million.

*Later on during the exploration process, we discovered few other issues:*

- Most counts were measured every 4 hours however there are some very peculiar count intervals, few were less than 20 minutes.  

- Some exact same stations were listed with different names probably capitalization error or typo, etc.
> For example, '14th ST' is equal to '14TH ST'.  

- Some stations have the same name but have different line names uncombined.

The above two issues were overlooked in the first place until we tried to group the data by station names and try to merge the MTA data with other datasets. However, both of them have skewed the graphs and outcome of our recommendation theory.

> **Some thoughts:**  
> Data cleaning is sometimes an overlooked while extremely important job of data scientists. Depending on how you deal with incomplete data and prepare the data, the outcome is always different.  
> For example, when we are trying to cue the daytime into bins, the bin size and start/end time significantly impacted our final outcome.

### String matching techniques


Other than pandas, especially to deal with the similar station name issue in the dataset, we has to explore other Python packages.

One of the noteworthy techniques is the FuzzyWuzzy Package. It uses Levenshtein Distance to calculate the differences between sequences and returns the most similar match in the given data lists. In our case, we used the dataset itself to find the closest match, which will be used to replace the string itself since they are essentially the same station name. The code is below:

```python
from fuzzywuzzy import fuzz
from fuzzywuzzy import process

names = df['COMPLETE_NAME'].unique()
with open('merge_mapping.csv', 'w') as f:
    for name in df_income['COMPLETE_NAME'].unique():
        # returns the best match
        score = process.extractOne(name, names, scorer=fuzz.token_sort_ratio)
        f.write(name + "," + score[0]+"\n")


with open('merge_mapping.csv') as namefile:
    reader = csv.reader(namefile)
    for row in reader:
       # use the most similiar match to replace itself
        df_income.loc[df_income.COMPLETE_NAME ==row[0], 'COMPLETE_NAME'] = row[1]
```


###Data visualizations and Theory building

Data visualization is a process with a lot of fun. It tells a story much faster than a simple head of the dataset. For example, below is a quick sort and plot of the top busiest stations in the morning peak hours and afternoon.


>As mentioned previously, the defenition of morning could significantly skewed the output here.   




![morning_rush](../assets/images/morning_rush.jpg)  

![afternoon_rush](../assets/images/afternoon_rush.jpg)  

The data for afternoon is actually a little bit skewed because:
- Afternoon subways are always much busier with tourists/students/people after work combined but morning peak time is most likely to be working class.
- People go to different places for eat or pleasure

-----------------------------
However, the graph of sum amount of passenger flow brought up **another issue**. For stations like time square that is generally busy, we can’t not determine whether it is actually a good place to focus since we do not want to talk to tourists in the end.  

To tackle with the issue of finding correct working places and residential places, we came up with an idea of looking at the exit over entry ratio. We used a scatter plot here to show the theory of how we finally select our favorite stations.  

![morning_rush](../assets/images/morning_entry.jpg)  

Basically ratio larger than 1 means more people exiting the station at a specific time period and vice versa.

![afternoon_rush](../assets/images/morning_exit.jpg)  

*This basically becomes a simplified version of classification plotting. By manually setting up threshold and breaking the whole dataset into four segments, we can then plot them on one figure with a much more obvious apperance. *

> **Some thoughts:**  
> The package we used to generate these figures is seaborn. It creates much prettier plots than the matploylib. Its functions are a little different even though I personanlly find it more understandable. Seaborn package becomes even handy to use in terms of automatically generating different colors with different data groups on one figure, which I have benefit in above example. However one problem I noticed is Seaborn is also harder to debug in the same time probably because it is built up on matplotlib.  


###Few things to note in the future projects

In the project, we spent a lot of time going back and forth on data cleaning and reploting. We keep finding new issues on the original dataset later on. And each one of the group would come up with total different outcome even we used agreed approach on the data. It is simply due to some changes on the variable we chose during this process. I believe this is a pretty common practice for all data scientists especially in team projects.  

> **A Note:**  
> A small tricky issue is even though Pandas is a very handy and quick dataframe, it often comes to a situation that you need to apply a certain function (using apply function) to every item in a column and this process can be very very slow. Therefore, optimizing the pandas operation is extremely helpful. An example would be, converting the date column into datetime type allows itself to do operation directly.

Another noteworthy insight came from the final group presentation. Most of the projects focused on making recommendation based on three or five subway stations, considering the limited labfor force. However, one of the groups came up with a list of top 50 stations, suggesting to rotate every week on this list for two months. The idea was that so it reaches out to different people in different locations.  

*Such idea was brought from the group member's previous working experience and appear to be a very solid point of view in terms of this project. Data science sometimes requires more than just statiscs or plotting. Domain knowledge could also be very important and it certainly helps to come up a better theory. And in afterwards working in building up models, domain knowledge could contribute to feature selecting, etc, which will definitely save a lot of work.*

---
layout: single
title: Database design and analysis of Yelp Challenge Dataset
modified:
categories: blog
tags: [SQL]
date: 2017-05-22T17:35:45-00:00
---

Recently I have done a lot of work in writing SQL scripts and building ETL pipelines for various projects. However, a common problem I encountered is that, when I was trying to read data from the databases, the process sometimes took so long that generating a single report becomes time-consuming. Plus, some databases will lock the session until the reading and updating is completed, which could cause problems if the database is a OLTP database where transactions took place frequently.

This is what motivates me to learn how to design a good database. In addition to that, I was also exploring what options there are to replace our current database that runs on local distributed machines. Thanks to Metis, I started to get in touch with AWS eco-systems and Redshift was the first thing I wanted to play with.

The dataset I used for this project was published by Yelp for its newest round of Challenge. The original purpose was to incentive researches into Image Classification Models. For more details, you can read it here at [yelp website](https://www.yelp.com/dataset_challenge).

Since this is a very practical data source, I used it as an example to explain how I will design the tables optimizing for analytics.

### Data Exploration

The raw dataset contains five json files, just like what you will get by calling Yelp's APIs.

Below is an example of the json files:

```python

{
    "business_id":"encrypted business id",
    "name":"business name",
    "neighborhood":"hood name",
    "address":"full address",
    "city":"city",
    "state":"state -- if applicable --",
    "postal code":"postal code",
    "latitude":latitude,
    "longitude":longitude,
    "stars":star rating, rounded to half-stars,
    "review_count":number of reviews,
    "is_open":0/1 (closed/open),
    "attributes":["an array of strings: each array element is an attribute"],
    "categories":["an array of strings of business categories"],
    "hours":["an array of strings of business hours"],
    "type": "business"
}
```

### Naive Tables

Apparently, the data was most likely stored in NoSQL database. This is a very typical industry practice which is to dump everything into NoSQL and then build ETL pipelines to transform data into a SQL database for analytics. This is exactly what I will simulate in later sections.

The first step of my task is to build a naive database where all the tables are designed exactly same as the data sources. Below is the graph.  

[graph](/assets/images/pre.png)

### Optimizing Tables  

The first thing I did is to normalize the tables based on the normal forms.

According to the first Norms, all the columns should only contain single information. In the original dataset, it looks like below:
```python

{'address': '227 E Baseline Rd, Ste J2',
 'attributes': ['BikeParking: True',
                'BusinessAcceptsBitcoin: False',
                'BusinessAcceptsCreditCards: True',
                "BusinessParking: {'garage': False, 'street': False, "
                "'validated': False, 'lot': True, 'valet': False}",
                'DogsAllowed: False',
                'RestaurantsPriceRange2: 2',
                'WheelchairAccessible: True'],
 'business_id': '0DI8Dt2PJp07XkVvIElIcQ',
 'categories': ['Tobacco Shops', 'Nightlife', 'Vape Shops', 'Shopping']
 }
```  

**There is no way we are going to store the categories as a string of all categories combined**. It will create a lot of mess later when we want to do a simple 'GROUP BY', say, on all the Restaurants with Nightlife. Therefore, a common practice is to break all these possible categories into different columns and use boolean value to represent whether this business belongs to that category. It will look like something below:

```

```

However, each business entity
[updated tables](/assets/images/optimized_db.png)

---
layout: single
title: Practical Tutorial of RNN in Tensorflow and Keras
modified:
categories: blog
excerpt: This is a practical tutorail intoducing the implementation of RNN, especially the Sequence to Sequence model in Tensorflow and Keras.
tags: [Tensorflow, Keras, RNN]
date: 2017-04-3T17:35:45-00:00
--- 


Today I am going to wrap up the project I did in last three weeks. I spent about two weeks trying to build a toy chatbot in both Keras(using TF as backend) and directly in TF. These two models work similar in their achitecture but they gave very different results and they are still very far away from actually being useful in any real world cases.  

To be honest, I am still far away from being an expert in the Recurrent Neural Networks but I did learn a lot of the fundamentals, ideas and implemented techinuqes regarding RNN. That's what motivates me to write down this practical guide of RNN.  

# Preparation  

## 1. Raw Data

I took a month of Reddit data and it is already **35G** decompressed. What I did originally is to split the dataset into few seperate CSV file, find the question and answer pair with a filter of max length 20 and min length 3. 

I wanted a smaller dataset to quickly come up with some good results but if I had more time I would try the model on the complete dataset. However, long sentences are always issues for training in terms of training time and complexity.   

## 2.Using Keras and Tensorflow 

Personally I think using Keras is productive but TensorFlow is better if you are constantly looking for advanced new models.  Keras is a very new library so you probably will not see a lot of new theories implemented in Keras. However, most scholars today use TensorFlow for their papers. For example, I had a very bad experience building myself a Sequence with Attention model in Keras (which is not available in Keras yet) but it has been around in Tensorflow for a while already.  
  
  >There does exist a package called [Seq2seq](https://github.com/farizrahman4u/seq2seq) written by Fariz Rahman trying to implement the Attention Model for Keras. However, it has a lot of bugs and has been out of maintainence for a while. Plus, it only supports Theano as backend as far as my personal experience.     
   
[Tensorflow Example Library](https://github.com/tensorflow/models) is a very active and complete resource for most advanced models in the industry(from papers published in last year or even this year). **Plus, Neural Networks take long time to train, sometimes up to months. It makes you feel much saver to work with some models that you are certain will work (All TF model examples have approved working model weights and published papers)!**
  
*To conclude, I would suggest a beginner to learn the fundamentals, building RNN examples in Tensorflow and then play with Keras later. If you are dealing with images, Keras is powerful and is easy to use*. 

# Data preparation  

The first step is to prepare the dataset as much as I can so the training process becomes easier later. There are four things I am able to prepare in advance:
1. Index to words. This is a python list of the vocabulary.
2. Word to index. This is a python dict of word:index. 
3. Questions can be stored as list of lists of word index (even though it will transformed back into one-hot encoder matrix later in Keras). 
4. Output, in this case being the answers, is supposed to be one-hot encoded 

## Buidling the dictionary  

To simplify the process, I removed the punctuations and lower all the characters in the dataset.  Still, there are over 200,000 unique words in the dataset.  
  
There are two reasons why we need to lower our dictionary size:  
  
  1. For most text understanding tasks, it is almost default that we use a embedding layer as the first layer. Even if we use Glove pretrained model, it will be a matrix of 2 x 10\*\*5 x 300 x batch-size, a matrix around 1GB if you put that in memory. Plus, the output is going to be one-hot encoded. The matrix is going be batch-size X sequence_length X vocab_size, which consumes another gigabyte of memory. **It is a good habit to mind check the memory size** because it just happens so often.

  2. There are so many words that are useless in terms of our conversational agent task. It creates a lot of noise and makes our model much harder to converge. In addition, remember the output of the model is going to be probabilities of each word. Having a huge dict is going to slow activation calcultion significantly.

  
Luckily, the existence of Zipf's law helped us to filter most of the useless words. Basically the law says the 2nd most frequent words appears about half of the times of the most frequent word appearing. **In my dataset, I used 10% cutoff, meaning I only kept the top 10% most frequent words, but when I filtered the original dataset, I still kept 97% of the content!** 
  
```python
from keras.preprocessing.text import Tokenizer
import string

punct = string.punctuation()
punct |= ('\n', '\r', '\t')
MAX_NB_WORDS = 20000

# this function split a sentence into a list of words.
tokenizer = Tokenizer(num_words=MAX_NB_WORDS,  filters=punct, lower=True, split=" ")
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# returns the list of words
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

```  
  
> Recently I read someone's work who does not train on word by word dictionary. Instead, he trained on character by character (like, a, b, c, ?, etc). Even if we consider the Upper case and all types of punctuations, there are at most 100 vocabs. However, as you can imagine, this type of method will require massive dataset and only converges after months.    

### Special Tokens for RNN

There is another import note in building the vocab. Almost every NLU task had to use four unique placeholders: <GO> <EOS> <UNK> <PAD>. Here is a quick explanation:  
- <GO> simply means it is a start of the sentence. It was often put at the begining of reponses as a signal of starting output.
- <EOS> means end of sentence. This is almost the 1st thing the model learns. Model learns to stop the output after some words. If you do not have this signal in your training data, most likely the model will try to fill up your sentences with random words.
- <UNK> this is a placeholder for those words not exist in the vocab. 
- <PAD> since most sequences are in varied length, we pad the sequence to a fixed length so the model can do batch computation.  
  
All these four symbols, except <PAD>, should be treated as regular words just like all other vocabs, which means they should have their unique embedding learned. **If you are using pretrained embedding, you must make sure they exit in your pretrained vocab!!!**

> Important Note! <PAD> should always be the 1st word in your vocab. This is required if you are going to Mask the <PAD> since they literally mean nothing.  

### Pad the sequence

The only reason you need to pad the sequence is because if you use the batch mode, all sequences must be of the same length inside your batch. Certainly you can get away with this by setting your batch-size to 1(gradient descent on each sample) but this will significantly lower the speed of going over one epoch.

Below is the code I used to pad the sequence.

```python
MAXLENGTH = 20
PAD = '<PAD>'

# Transform a list of num_samples sequences (lists of word indices) into a 2D Numpy array of shape 
padded = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAXLENGTH, dtype='int32',padding='post', truncating='post', value=PAD)
```

# Model Building

Building a complete model in Keras and Tensorflow is essentially the same if you understood the basic elements of a typical RNN model.  

Think of building a model as drawing a graph where the input data flows to the end. Keras made things easier in terms of simplifying the process of declaring Input and Output matrix type, gradient descent and back propagating. In Tensorflow, all these need to be set up manually.  



## Embedding layer

Simple put, the Embedding layer is a simple matrix multiplication that transforms words into their corresponding word embeddings, or in other words, this is where model learns the meaning of each word. Even though our input is a list of integers, but Keras will transform it into a one-hot matrix in order to quickly do this as a matrix multiplication, a one-hot matrix.

To gain a better result and save some training time, it is helpful to train the embeddings with an autoencoder and then use them as initialization of the Embedding layer for the RNN model. 

```python

EMBEDDING_DIM = 1024
MAX_SEQUENCE_LENGTH = 20
KEEP_PROB = 0.2

# this is a typical Keras Embedding Layer
# for masking, please refer to below section
embedding_layer = Embedding(num_words,
                            EMBEDDING_DIM,
                            weights=[pretrained_embedding], # remove this if you want to train your weight
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False,
                            masking=True)

#----------------------------------------------------
# this is what a typical Tensorflow Embedding Layer looks like
with tf.device("/cpu:0"): # embedding calcualtion can be done on CPU. 
    # Create new variable named 'embedding' to connect the character input to the base layer of the RNN. 
    #The function tf.get_variable() is used to get or create a variable instead of a direct call to tf.Variable

    embedding = tf.get_variable("embedding", [num_words, EMBEDDING_DIM])
    # Create an embedding tensor with tf.nn.embedding_lookup(embedding, self.input_data).
    # This tensor has dimensions batch_size x seq_length x rnn_size.
    inputs = tf.nn.embedding_lookup(embedding, input_data)

    if KEEP_PROB < 1:
        inputs = tf.nn.dropout(inputs, KEEP_PROB)
```

## Basic LSTM/GRU Layer

Simply put, LSTM or GRU(a little variation to LSTM) is a layer that has two  inputs/outputs. One of them is called 'state', representing what was happeneing in last Timestamp and the other one is called 'Gate', deciding what should be kept and forgot at current layer (*this is very naive explanation and is very flawed, [click me for more details](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)*). We keep feeding both the state and info we learned so far to the cell itself to generate new state and new info. 

Practically I found GRU is faster to train and converge (GRU has one less gate than LSTM) and they show no apparant difference in terms of outcome accuracy, etc.

> In Keras, if you want to stack LSTM for few times, you need to add a special argument in first few layers except the last one: return_sequences=True. What it does is to return the state and info cell in sequences. It is set to False in default and if it is False, the LSTM cell only return the state of the last Timestamp.

```python

DROP_OUT_RATE = 0.2

from keras.models import Sequential
from keras.layers import LSTM, GRU

# this is a LTSM layer in Keras, you can replace LSTM with GRU
# Remember KERAS requires the 1st layer to declare the input shape
model = Sequential()
model.add(Embedding(num_words, EMBEDDING_DIM, input_shape=(?,?)))
# if you want more layers, just repeat below for few times
model.add(LSTM(hidden_nr, dropout=DROP_OUT_RATE, recurrent_dropout=DROP_OUT_RATE, return_sequences=True))
model.add(LSTM(hidden_nr, dropout=DROP_OUT_RATE, recurrent_dropout=DROP_OUT_RATE))


#----------------------------------------------------
# This is a LTSM layer in Tensorflow. Replacing Basic LSTMCell with GRU
# or LSTM cell works the same way.

import inspect
import tensorflow as tf
from tensorflow.contrib.rnn import BasicLSTMCell, LTSMCell, GRUCell

NUM_STEPS = 5
HIDDEN_SIZE = 1024

def lstm_cell():
      # With the latest TensorFlow source code (as of Mar 27, 2017),
      # the BasicLSTMCell will need a reuse parameter which is unfortunately 
      # not defined in TensorFlow 1.0. To maintain backwards compatibility, we 
      # add an argument check here:
    if 'reuse' in inspect.getargspec(
        tf.contrib.rnn.BasicLSTMCell.__init__).args:
        return tf.contrib.rnn.BasicLSTMCell(
            HIDDEN_SIZE, forget_bias=0.0, state_is_tuple=True,
            reuse=tf.get_variable_scope().reuse)
    else:
        # There are other choices such 
        return tf.contrib.rnn.BasicLSTMCell(
            HIDDEN_SIZE, forget_bias=0.0, state_is_tuple=True)

# drop out is a techinque of regularization,
# you can use either previous one or this one
def attn_cell():
    return tf.contrib.rnn.DropoutWrapper(
            lstm_cell(), output_keep_prob=keep_prob)

# MultiRNNCell is a TF built-in wrapper for you 
# to quickly build stacked RNN cells
cell = tf.contrib.rnn.MultiRNNCell(
        [attn_cell() for _ in range(num_layers)],
         state_is_tuple=True)

# for TF, you have to declare all the input by yourself
# remember we have two inputs, one is the state, one is the info
_initial_state = cell.zero_state(batch_size, tf.float32)
outputs = []
state = self._initial_state

with tf.variable_scope("RNN"):
    for time_step in range(NUM_STEPS):
        if time_step > 0: 
            tf.get_variable_scope().reuse_variables()
        (cell_output, state) = cell(inputs[:, time_step, :], state)
        outputs.append(cell_output)

```

## Decoder and Loss

Decoder is another set of RNN layers stacked over each other. **The idea of RNN decoder(regardless of TF or Keras)is to us your previous output as next input. The final output will then become a list of outputs at different time stamp. This is implemented slightly different in Keras and Tensorflow and this is why we are able to concate the output from different timestamp and do batch calculation all in once.**. Below is a graph of the basic decoder.  

![RNN Model](/assets/images/RNNmodel.png)  

There are many other techniques, such as Peeking-Decoder, Decoder with attention, etc. However, as I have mentioned above, they were not implemented in Keras yet. Even in Tensorflow, you are most likely to find these techniques all over the place in TF folder.


```python

from keras.layers import Layer, Dense, RepeatVector
from keras.layers import Activation, Dropout, Embedding, TimeDistributed
from keras.optimizers import Adam

MAX_LENGTH =20
EMBED_SIZE = 1024
HIDDEN_SIZE = 1024
# This is an example of Keras encoder and decoder.
model.add(Embedding(vocab_size,
                        EMBED_SIZE, #
                        input_length=MAX_LENGTH,
                        name='embed',
                        mask_zero=True)) 
model.add(LSTM(HIDDEN_SIZE, return_sequences=True))                        
model.add(LSTM(HIDDEN_SIZE))
# LSTM returns the embedding info at the last Timestamp
# we repeat the info for n times, n is the sequence length
# of target output. To generate the output in sequence,
# output at each timestamp was fed into LSTM cell as input. 
# That's why I used few LSTM cells returning sequences stacked
# over each other.
model.add(RepeatVector(MAX_LENGTH))
model.add(LSTM(HIDDEN_SIZE, return_sequences=True))
model.add(LSTM(HIDDEN_SIZE, return_sequences=True))
# Use TimeDistributed Layer to catch the output at each
# timestamp, namely the words in sequence in our case.
model.add(TimeDistributed(Dense(vocab_size)))
model.add(Activation('softmax'))
# this is the clipping norm, read below section for details
adam = Adam(clipnorm=5.) 
# ...Compile it...
model.compile(
        optimizer='adam', # we use Adam Optimizer
        # and categorical entrophy for loss
        loss='categorical_crossentrophy', 
        )

# Till now, a complete model in Keras is set up.


#-----------------------Tensorflow Example----------------------------------

# Continue with previous example.

# Note the following comment in rnn_cell.py:
#   Note: in many cases it may be more efficient to not use this wrapper,
#   but instead concatenate the whole sequence of your outputs in time,
#   do the projection on this batch-concatenated sequence, then split it
#   if needed or directly feed into a softmax.

# tf.concat concatenates the output tensors along the rnn_size dimension,
# to make a single tensor of shape [batch_size x (seq_length * rnn_size)].

# Concat output:  [(rnn output: batch 0, seq 0) (rnn output: batch 0, seq 1) ... (rnn output: batch 0, seq seq_len-1)]

# Reshape Output:       
#   [rnn output: batch 0, seq 0]
#   [rnn output: batch 0, seq 1]
#   ...
#   [rnn output: batch 0, seq seq_len-1]

output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, HIDDEN_SIZE])

# Create new variable softmax_w and softmax_b for output.
# softmax_w is a weights matrix from the top layer of the model (size of hidden nuerons)
# to the vocabulary output (of size vocab_size).
softmax_w = tf.get_variable(
        "softmax_w", [HIDDEN_SIZE, vocab_size], dtype=tf.float32)
# softmax_b is a bias vector of the ouput characters (of size vocab_size).
softmax_b = tf.get_variable("softmax_b", [vocab_size], dtype=tf.float32)
# Obtain logits node by applying output weights and biases to the output tensor.
# Logits is a tensor of shape [(batch_size * seq_length) x vocab_size].
# Recall that outputs is a 2D tensor of shape [(batch_size * seq_length) x rnn_size],
# and softmax_w is a 2D tensor of shape [rnn_size x vocab_size].
# The matrix product is therefore a new 2D tensor of [(batch_size * seq_length) x vocab_size].
# In other words, that multiplication converts a loooong list of rnn_size vectors
# to a loooong list of vocab_size vectors.
# Then add softmax_b (a single vocab-sized vector) to every row of that list.
# That gives you the logits!
logits = tf.matmul(output, softmax_w) + softmax_b

```

## Loss, Gradient Descent, and Optimizer

- Categorical Cross Entrophy is a good choice for Classfication problem and it is often used in Keras for translation task.  

>Categorical Cross Entrophy: L = -sum(y * log(y_prediction)) where y is the probability distribution of true labels (typically a one-hot vector) and y_prediction is the probability distribution of the predicted labels, often coming from a softmax.  

In Tensorflow, most RNN tasks use built-in Sequence_loss Function to calculate the loss (**it is essentially Weighted cross-entropy loss for a sequence of logits**). 

- Gradient Descent is also critical to model performance. It is important to choose the batch size for gradient descent. Basically, the smaller batch size you choose, the longer time one epoch is going to take and the more likely your model is going to overfit. I used batch-size of 20 for my chatbot. Also, GD is much easier handled in Keras than Tensorflow.

>If you are dealing with sequences in different length, instead of padding them into fixed length (which is required for batch calculation), you can choose to set batch size to 1 so the model does gradient descent per each sample point.

- Most of translation tasks or conversational bot in our case, used Adam Optimizer these days. One of the concerns regarding Adam Optimizer is that it tends to overfit. Personally I have not even got close to overfit in such task but it maybe true for tasks like RNN classfication, etc. RMSprop is also said to be another good choice. I have tried both and they do not show much difference to me. 

>**Gradient Clipping** is a technique to prevent exploding gradients in very deep networks, typically RNN. There exist various ways to perform gradient clipping, but the a common one is to normalize the gradients of a parameter vector when its L2 norm exceeds a certain threshold according to new_gradients = gradients * threshold / l2_norm(gradients). [more info](http://jmlr.csail.mit.edu/proceedings/papers/v28/pascanu13.pdf)  

- Clipping was passed in as an argument to the Optimizer in Keras. We will be more clear on this process in below Tensorflow Example.

```python
# there is no more code need to be done for Keras
# below is the example of Tensorflow
LEARNING_RATE = 6e-5
BATCH_SIZE =20
SEQ_LENGTH = 20
# CONTINUEING PREVIOUS SECTION..
loss = seq2seq.sequence_loss_by_example([logits], # logits: 1-item list of 2D Tensors of shape [batch_size x vocab_size]
        [tf.reshape(targets, [-1])], # targets: 1-item list of 1D batch-sized int32 Tensors of the same length as logits
        [tf.ones([batch_size * seq_length])], # weights: 1-item list of 1D batch-sized float-Tensors of the same length as logits
        vocab_size) # num_decoder_symbols: integer, number of decoder symbols (output classes)

#-------Simplified Version-------------
# You can just ask TF to minimize the loss for you

# train_op = tf.train.RMSPropOptimizer(learning_rate=lr).minimize(self.loss)  

#--------Complete Version--------------      
# Cost is the arithmetic mean of the values of the loss tensor
# (the sum divided by the total number of elements).
# It is a single-element floating point tensor. This is what the optimizer seeks to minimize.
cost = tf.reduce_sum(loss) / BATCH_SIZE / SEQ_LENGTH
# Create a summary for our cost.
tf.summary.scalar("cost", cost)
# Create a node to track the learning rate as it decays through the epochs.
lr = tf.Variable(LEARNING_RATE, trainable=False)
global_epoch_fraction = tf.Variable(0.0, trainable=False)
global_seconds_elapsed = tf.Variable(0.0, trainable=False)
tvars = tf.trainable_variables() # tvars is a python list of all trainable TF Variable objects.

# tf.gradients returns a list of tensors of length len(tvars) where each tensor is sum(dy/dx).
grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),grad_clip)
optimizer = tf.train.AdamOptimizer(lr) # Use ADAM optimizer with the current learning rate.
# Zip creates a list of tuples, where each tuple is (variable tensor, gradient tensor).
# Training op nudges the variables along the gradient, with the given learning rate, using the ADAM optimizer.
# This is the op that a training session should be instructed to perform.
self.train_op = optimizer.apply_gradients(zip(grads, tvars))
self.summary_op = tf.merge_all_summaries()
```

## Attention/Bidirectional LSTM

[Here](http://distill.pub/2016/augmented-rnns/) is a very good explanation on so called Augmented RNNs. 
Generally, Attention Model is an extra layer that helps the model to carry long-term memory of focus on certain part of the sentence. Below are examples of how they are implemented in Keras and Tensorflow.

```python

#Keras TO-DO

#--------TensorFlow Example---------------

from tf.contrib.rnn import AttentionCellWrapper

# TO-DO
```

Bidirectional LSTM came from a [study](https://www.cs.toronto.edu/~graves/asru_2013.pdf) that shows input information from the past and future of the current time frame can be used unlike standard RNN which requires the delays for including future information.

```python

# TO-DO
```

# Training

Till now, we have built models in both Keras and Tensorflow. Training in Keras is merely calling train function. However, in Tensorflow, it is much more complicated.

The basic workflow in Tensorflow:
- Create a session where you run the model
- Specify the Inputs and Outputs, both of which live on the Session we created. Output is a list of items we would need, including:
  -Training Loss
  -Inner State of the model
  -Current Learning Rate optimized by the optimizer
  -Summary of the model
- We manually log the epoch and every batch so we can resume if we stopped the trainning process
- For every Epoch, reset the Initiate State (but not restting the weights, etc)
- For every batch, assign the new Learning Rate(optimized by the optimizer) and carry over the initiate state from last Batch, run the session which will calcuate the loss and do Gradient Descent again. 

```python


#-----------------TensorFlow Example------------------------

SAVE_EVERY = 100 # save the model every 100 batches
TOTAL_BATCH_COUNT = ? # you should calculate this based on your data size
DECAY_STEPS = 100000
DECAY_RATE = 0.95

 # Create the model! Assume you have Model Class
 # that built up the model
model = Model(args)

config = tf.ConfigProto(log_device_placement=False)
# this is to allow your machine to use more GPU
# space as needed
config.gpu_options.allow_growth = True

with tf.Session(config=config) as sess:
    # first, you need to initiate all the variables
    # Same thing is required if you loaded a pretrained model
    tf.initialize_all_variables().run()
    # create a saver object
    saver = tf.train.Saver(model.save_variables_list())
    if (load_model):
        print("Loading saved parameters")
        saver.restore(sess, ckpt.model_checkpoint_path)
    global_epoch_fraction = sess.run(model.global_epoch_fraction)
    global_seconds_elapsed = sess.run(model.global_seconds_elapsed)

    initial_batch_step = int((global_epoch_fraction
                - int(global_epoch_fraction)) * TOTAL_BATCH_COUNT)
    epoch_range = (int(global_epoch_fraction),
                num_epochs + int(global_epoch_fraction))

    # specify the output 
    outputs = [model.cost, model.final_state, model.train_op, model.summary_op]
    # if model is lstm, we need to load both hidden status and info carried
    is_lstm = (model == 'lstm')

    global_step = epoch_range[0] * TOTAL_BATCH_COUNT + initial_batch_step
    try:
        for e in xrange(*epoch_range):
            # e iterates through the training epochs.
            # Reset the model state, so it does not carry over from the end of the previous epoch.
            state = sess.run(model.initial_state)
            batch_range = (initial_batch_step, TOTAL_BATCH_COUNT)
            initial_batch_step = 0
            for b in xrange(*batch_range):
                global_step += 1
                # every 100k steps, we decay the Learning Rate
                if global_step % DECAY_STEPS == 0:
                    # Set the model.lr element of the model to track
                    # the appropriately decayed learning rate.
                    current_learning_rate = sess.run(model.lr)
                    current_learning_rate *= DECAY_RATE
                    # updating learning rate of the model
                    sess.run(tf.assign(model.lr, current_learning_rate))
                    print("Decayed learning rate to {}".format(current_learning_rate))
                    start = time.time()
                    # Pull the next batch inputs (x) and targets (y) from the data loader (you should have a generator ready for training).
                    x, y = data_loader.next_batch()

                    # feed is a dictionary of variable references and respective values for initialization.
                    # Initialize the model's input data and target data from the batch,
                    # and initialize the model state to the final state from the previous batch, so that
                    # model state is accumulated and carried over between batches.
                    feed = {model.input_data: x, model.targets: y}
                    if is_lstm:
                        # for LSTM model, there are two states we need to 
                        # carry over
                        for i, (c, h) in enumerate(model.initial_state):
                            feed[c] = state[i].c
                            feed[h] = state[i].h
                    else:
                       # if you are certain you are just using LSTM, just use previous one and discard below
                        for i, c in enumerate(model.initial_state):
                            feed[c] = state[i]
                    # Run the session! Specifically, tell TensorFlow to compute the graph to calculate
                    # the values of cost, final state, and the training op.
                    # Cost is used to monitor progress.
                    # Final state is used to carry over the state into the next batch.
                    # Training op is not used, but we want it to be calculated, since that calculation
                    # is what updates parameter states (i.e. that is where the training happens).
                    train_loss, state, _, summary = sess.run(outputs, feed)
                    elapsed = time.time() - start
                    global_seconds_elapsed += elapsed
                    print "{}/{} (epoch {}/{}), loss = {:.3f}, time/batch = {:.3f}s" \
                        .format(b, batch_range[1], e, epoch_range[1], train_loss, elapsed)
                    # Every save_every batches, save the model to disk.
                    # By default, only the five most recent checkpoint files are kept.
                    if (e * batch_range[1] + b + 1) % SAVE_EVERY == 0 \
                            or (e == epoch_range[1] - 1 and b == batch_range[1] - 1):
                        save_model(sess, saver, model, args.save_dir, global_step,
                                data_loader.total_batch_count, global_seconds_elapsed)
        except KeyboardInterrupt:
            # Introduce a line break after ^C is displayed so save message
            # is on its own line.
            print()
        finally:
            writer.flush()
            global_step = e * TOTAL_BATCH_COUNT + b
            save_model(sess, saver, model, args.save_dir, global_step,
                    TOTAL_BATCH_COUNT, global_seconds_elapsed)

```


# Wrap Up


# Appendix

1. Keras giving NaN as accuracy or loss?
  You should try to add clipping function to Optimizer. If it doesn't work,  set your target variable to data type of Numpy.Float64 or higher instead of Numpy.Bool. Last solution, increase your memory size or lower your matrix dimension. 

>It takes really long time for me to install GloVE on Mac. The reason turns out to be Mac default c language complie is clang, but we need to set it to the recent version of gcc, which usually stores at **/usr/local/bin/gcc-6**.  
The easiest way is to:  

```command
cd /usr/local/bin
ln -s gcc-6
which gcc
gcc --version
```


To set up correct jupyter notebook env on AWS

```command
pip install ipykernel
python -m ipykernel install --user --name ENVNAME --display-name "Python (whatever you want to call it)"
```
